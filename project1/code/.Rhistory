housing_data <- read.csv("housing_data.csv")
network_backup_dataset <- read.csv("network_backup_dataset.csv")
View(network_backup_dataset)
View(network_backup_dataset)
View(housing_data)
View(housing_data)
housing_data <- read.csv("housing_data.csv")
network_backup_dataset <- read.csv("network_backup_dataset.csv")
install.packages("ggplot2")
library('ggplot2')
install.packages("lattice")
library('lattice')
install.packages("DAAG")
library('DAAG')
install.packages("randomForest")
library('randomForest')
install.packages("neuralnet")
library("neuralnet")
#install.packages("usdm")
#library('usdm')
install.packages("MASS")
library('MASS')
install.packages("glmnet")
library('glmnet')
install.packages("lars")
library('lars')
install.packages("data.table")
library(data.table)
install.packages("gtools")
library(gtools)
#converting days of the week into integer values
dayList <- levels(network_backup_dataset$Day.of.Week)[c(2,6,7,5,1,3,4)]
numeric_day <- as.numeric(factor(network_backup_dataset$Day.of.Week,dayList))
#appending the numberic day column to original dataset
network_backup_dataset$numeric.day <- numeric_day
#calculating the day number depending on the week number
day_number = ((network_backup_dataset$Week.. -1)*7)+network_backup_dataset$numeric.day
#appending the day number to original dataset
network_backup_dataset$day.number <- day_number
#create a new dataset only for 20 days
new_dataset = network_backup_dataset[with(network_backup_dataset,day.number >= 21 & day.number <=40), ]
#sort the new dataset with respect to workflow id
new_dataset = new_dataset[order(new_dataset$Work.Flow.ID),]
#segregate data according to workglow
workflow0_filesize = new_dataset[with(new_dataset,Work.Flow.ID == 'work_flow_0'), ]
workflow1_filesize = new_dataset[with(new_dataset,Work.Flow.ID == 'work_flow_1'), ]
workflow2_filesize = new_dataset[with(new_dataset,Work.Flow.ID == 'work_flow_2'), ]
workflow3_filesize = new_dataset[with(new_dataset,Work.Flow.ID == 'work_flow_3'), ]
workflow4_filesize = new_dataset[with(new_dataset,Work.Flow.ID == 'work_flow_4'), ]
dataframe0 <- data.frame(workflow0_filesize)
DT <- data.table(workflow0_filesize)
sum1 <- DT[,sum(Size.of.Backup..GB.), by = day.number]
dataframe0 <- data.frame(workflow1_filesize)
DT <- data.table(workflow1_filesize)
sum2 <- DT[,sum(Size.of.Backup..GB.), by = day.number]
dataframe0 <- data.frame(workflow2_filesize)
DT <- data.table(workflow2_filesize)
sum3 <- DT[,sum(Size.of.Backup..GB.), by = day.number]
dataframe0 <- data.frame(workflow3_filesize)
DT <- data.table(workflow3_filesize)
sum4 <- DT[,sum(Size.of.Backup..GB.), by = day.number]
dataframe0 <- data.frame(workflow4_filesize)
DT <- data.table(workflow4_filesize)
sum5 <- DT[,sum(Size.of.Backup..GB.), by = day.number]
#plot the graph
ggplot(sum1, aes(x = day.number))+ geom_line(aes(y = V1),color="blue") +
labs(x="Number of Days", y="Size of Backup (GB)") +
scale_x_continuous(breaks = seq(1,20,1)) + ggtitle("Workflow 0")
ggplot(sum2, aes(x = day.number))+ geom_line(aes(y = V1),color="blue") +
labs(x="Number of Days", y="Size of Backup (GB)") +
scale_x_continuous(breaks = seq(1,20,1))  + ggtitle("Workflow 1")
ggplot(sum3, aes(x = day.number))+ geom_line(aes(y = V1),color="blue") +
labs(x="Number of Days", y="Size of Backup (GB)") +
scale_x_continuous(breaks = seq(1,20,1))  + ggtitle("Workflow 2")
ggplot(sum4, aes(x = day.number))+ geom_line(aes(y = V1),color="blue") +
labs(x="Number of Days", y="Size of Backup (GB)")   +
scale_x_continuous(breaks = seq(1,20,1))  + ggtitle("Workflow 3")
ggplot(sum5, aes(x = day.number))+ geom_line(aes(y = V1),color="blue") +
labs(x="Number of Days", y="Size of Backup (GB)") +
scale_x_continuous(breaks = seq(1,20,1))  + ggtitle("Workflow 4")
################################################################################
initial_model <- lm(Size.of.Backup..GB. ~ Week.. + Day.of.Week + Backup.Start.Time...Hour.of.Day + Work.Flow.ID+File.Name+Backup.Time..hour.,network_backup_dataset)
summary(initial_model)
rmse_initial <- sqrt(sum((network_backup_dataset$Size.of.Backup..GB.-initial_model$fitted.values)^2)/nrow(network_backup_dataset))
# higher the t-value, better the parameter. Therefore we choose the best 3 parameters : Backuptime, days, startTime
regression_model <- lm(Size.of.Backup..GB. ~ Day.of.Week+Backup.Start.Time...Hour.of.Day+Backup.Time..hour.,network_backup_dataset)
summary(regression_model)
rmse_main <- sqrt(sum((network_backup_dataset$Size.of.Backup..GB.-regression_model$fitted.values)^2)/nrow(network_backup_dataset))
plot(regression_model)
folds <- cut(seq(1,nrow(shuffled_data)),breaks=10,labels=FALSE)
predicted_values = matrix(data=NA,nrow=nrow(network_backup_dataset),ncol=1)
#Perform 10 fold cross validation
for(i in 1:10){
#separate test and trainign data using which
test_indices <- which(folds==i,arr.ind=TRUE)
testData <- shuffled_data[test_indices, ]
trainData <- shuffled_data[-test_indices, ]
test_model <- lm(Size.of.Backup..GB. ~ Day.of.Week+Backup.Start.Time...Hour.of.Day+Backup.Time..hour.,trainData) #Train the Model on 90% of the data
test_prediction <- predict(test_model,testData) #Test on the% of the data
row_names <- rownames(as.matrix(test_prediction))
for(j in 1:length(test_prediction))
{
predicted_values[as.numeric(row_names[j])] = test_prediction[j] #Store the predicted values
}
}
actual_values <- network_backup_dataset$Size.of.Backup..GB.
cv_rmse <- sqrt(sum((actual_values-predicted_values)^2)/nrow(network_backup_dataset)) #Calculate the RMSE = 0.7409
################################################################################
#Fitted Values and Actual Values Scatter Plot over Time
library(lattice)
dayList <- levels(network_backup_dataset$Day.of.Week)[c(2,6,7,5,1,3,4)]
numeric_day <- as.numeric(factor(network_backup_dataset$Day.of.Week,dayList))
network_backup_dataset$numeric.day <- numeric_day
res <- matrix(network_backup_dataset$numeric.day,nrow = length(network_backup_dataset$numeric.day))
res <- cbind(res, x2 = network_backup_dataset$Backup.Start.Time...Hour.of.Day)
res <- cbind(res, x3 = network_backup_dataset$Backup.Time..hour.)
res <- cbind(res, Observed = network_backup_dataset$Size.of.Backup..GB. )
res <- cbind(res, Predicted = predicted_values)
res <- data.frame(res)
xyplot(c(res$Observed,as.double(res$V5)) ~ res$V1, data = res,  auto.key = TRUE,xlab='Week',ylab='Observed Backup Size in GB')
xyplot(res$V5 ~ res$V1, data = res,  auto.key = TRUE,xlab='Week',ylab='Backup Size in GB')
#xyplot(network_backup_dataset$Size.of.Backup..GB. ~ res[,1], data = res, type = c("p","r"), col.line = "red")
plot(res$V1,res$Observed)
plot(res$V1, res$V5,xlab='Week',col = 4)
points(res$V1,res$Observed, col = 2)
################################################################################
#Residuals versus Fitted Values Plot
library('ggplot2')
qplot(y=(actual_values-predicted_values),x=predicted_values,xlab='Fitted Values',ylab='Residuals')
################################################################################
#Cross Validation using in-built function
library('DAAG')
cv_model<-cv.lm(data = network_backup_dataset, form.lm = formula
(Size.of.Backup..GB. ~ Day.of.Week+Backup.Start.Time...Hour.of.Day+Backup.Time..hour.),
m = 10)
predicted_values <- cv_model$cvpred
actual_values <- network_backup_dataset$Size.of.Backup..GB.
cv_model_inbuilt_rmse <- sqrt(sum((actual_values-predicted_values)^2)/nrow(network_backup_dataset)) #RMSE = 07407
colnames(housing_data) <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV");
## Linear regression
library('usdm')
#choosing the best parameters
#show summary in report
#explain significance of t-value
model1 <- lm(MEDV~.,data=housing_data)
sm1 <- summary(model1)
rmse1 <- sqrt(sum((housing_data$MEDV-model1$fitted.values)^2)/nrow(housing_data))
#taking parameters with highest t-value (shown with 3 starts in summary)-> these are ZN,NOX,RM,DIS,RAD,PTRATIO,B,LSTAT
model2 <- lm(MEDV ~ ZN+NOX+RM+DIS+RAD+PTRATIO+B+LSTAT,data=housing_data)
sm2 <- summary(model2)
rmse2 <- sqrt(sum((housing_data$MEDV-model2$fitted.values)^2)/nrow(housing_data))
#fine tuning the model further-> taking only most important parameters from model2-> NOX,RM,DIS,PTRATIO,B,LSTAT
model3 <- lm(MEDV ~ NOX+RM+DIS+PTRATIO+B+LSTAT,data=housing_data)
sm3 <- summary(model3)
rmse3 <- sqrt(sum((housing_data$MEDV-model3$fitted.values)^2)/nrow(housing_data))
################################################################################
#cross validation
sampledData<-housing_data[sample(nrow(housing_data)),]
#creating 10 folds
folds <- cut(seq(1,nrow(sampledData)),breaks=10,labels=FALSE)
fitted_values = matrix(data=NA,nrow=nrow(housing_data),ncol=1)
residual_values = matrix(data=NA,nrow=nrow(housing_data),ncol=1)
actual_values <- housing_data$MEDV
#Performing 10 fold cross validation
for(fold in 1:10){
test_indices <- which(folds==fold,arr.ind=TRUE)
testData <- sampledData[test_indices, ]
trainData <- sampledData[-test_indices, ]
tempModel <- lm(MEDV ~ NOX+RM+DIS+PTRATIO+B+LSTAT,trainData)
predicted_values <- predict(tempModel,testData)
rows <- rownames(as.matrix(predicted_values))
for(j in 1:length(predicted_values))
{
fitted_values[as.numeric(rows[j])] = predicted_values[j]
#residual_values[as.numeric(rows[j])] = actual_values[as.numeric(rows[j])] - predicted_values[j]
}
}
rmse <- sqrt(sum((actual_values-fitted_values)^2)/nrow(housing_data)) # RMSE
plot(tempModel)
library('DAAG')
cv_model<-cv.lm(data = housing_data, form.lm = formula
(MEDV ~ NOX+RM+DIS+PTRATIO+B+LSTAT),
m = 10) # plots predicted vs actual values for every fold internally, so just used it off the shelf
plot(x = fitted_values, y = actual_values)
###########################################################################################
#                                     POLYNOMIAL REGRESSION                               #
###########################################################################################
shuffled_data<-housing_data[sample(nrow(housing_data)),] #Randomly distributing the data
#Create 10 equally size folds
folds <- cut(seq(1,nrow(shuffled_data)),breaks=10,labels=FALSE)
all_rmse = NULL
#10 fold cross validation
for(deg in 1:10)
{
fitted_values = matrix(data=NA,nrow=nrow(housing_data),ncol=1)
rmse = NULL
for(fold in 1:10){
test_indices <- which(folds==fold,arr.ind=TRUE)
trainData <- (shuffled_data[-test_indices, ])
pr_model <- lm(MEDV ~ polym(NOX,RM,DIS,PTRATIO,B,LSTAT,degree = deg,raw = TRUE),data=trainData)
testData <- shuffled_data[test_indices, ]
predicted_values <- predict(pr_model,testData)
name <- rownames(as.matrix(predicted_values))
for(j in 1:length(predicted_values))
{
fitted_values[as.numeric(name[j])] = predicted_values[j]
}
}
actual_values <- housing_data$MEDV
rmse <- sqrt(mean((actual_values-fitted_values)^2))
all_rmse <- append(all_rmse,rmse)
}
colnames(housing_data) <- c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV");
## Linear regression
library('usdm')
install.packages("usdm")
library(usdm)
